#! /usr/bin/env python3

"""MAPO running script."""

# pylint: disable=import-self, no-name-in-module

import argparse
import json
import ray
from ray import tune
from mapo import register_all_agents, register_all_environments
from mapo.version import __version__


def read_config_json(filepath):
    """Load JSON object from file."""
    with open(filepath, "r") as file:
        return json.loads(file.read())


def parse_args():
    """Parse command-line arguments."""
    # pylint: disable=too-many-statements
    description = "Model-Aware Policy Optimization ({})".format(__version__)
    usage = "%(prog)s --run {MAPO,OffMAPO} --env ENV [ENV ...] [CONFIG-OPTIONS]"

    parser = argparse.ArgumentParser(usage=usage, description=description)

    parser.add_argument(
        "--run",
        type=str,
        choices=["MAPO", "OffMAPO", "OurTD3"],
        required=True,
        help="The algorithm or model to train.",
    )
    parser.add_argument(
        "--env",
        type=str,
        nargs="+",
        default=[],
        required=True,
        help="The gym environment to use.",
    )
    parser.add_argument(
        "--verbose",
        type=int,
        choices=[0, 1, 2],
        default=2,
        help="0, 1, or 2. Verbosity mode. 0 = silent, "
        "1 = only status updates, 2 = status and trial results "
        "(default=2)",
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        default=False,
        help="Debugging flag (default=%(default)s).",
    )

    mapo_args_group = parser.add_argument_group(">> MAPO")
    mapo_args_group.add_argument(
        "--kernel",
        type=str,
        choices=["l1", "l2", "linf", "dot-product", "cos-similarity"],
        default="l2",
        help="Kernel metric to compare gradients in MAPO. (default=%(default)s)",
    )
    mapo_args_group.add_argument(
        "--use-true-dynamics",
        action="store_true",
        default=False,
        help="Flag to use the true dyamics log prob in the gradient estimator "
        "(default=%(default)s)",
    )
    mapo_args_group.add_argument(
        "--branching-factor",
        type=int,
        nargs="+",
        default=[32],
        help="The number of virtual samples used in MAPO gradient "
        "(default=%(default)s)",
    )
    mapo_args_group.add_argument(
        "--madpg-estimator",
        type=str,
        nargs="+",
        choices=["sf", "pd"],
        default="[sf]",
        help="The model-aware policy gradient estimator (default=%(default)s)",
    )
    mapo_args_group.add_argument(
        "--model-loss",
        type=str,
        choices=["mle", "pga"],
        default="pga",
        help="The loss function used for learning the dynamics model "
        "(default=%(default)s)",
    )

    model_args_group = parser.add_argument_group(">> Models")
    model_args_group.add_argument(
        "--config-actor-net",
        type=read_config_json,
        help="The actor network configuration JSON file.",
    )
    model_args_group.add_argument(
        "--config-critic-net",
        type=read_config_json,
        help="The critic network configuration JSON file.",
    )
    model_args_group.add_argument(
        "--config-dynamics-net",
        type=read_config_json,
        help="The dynamics network configuration JSON file.",
    )

    optimizers_args_group = parser.add_argument_group(">> Optimization")
    optimizers_args_group.add_argument(
        "--apply-gradients",
        type=str,
        choices=["delayed", "sgd_iter"],
        default="sgd_iter",
        help="Strategy for applying gradients during optimization "
        "(default=%(default)s).",
    )
    optimizers_args_group.add_argument(
        "--num-sgd-iter",
        type=int,
        default=80,
        help="The number of SGD optimizer iterations (default=%(default)s).",
    )
    optimizers_args_group.add_argument(
        "--critic-sgd-iter",
        type=int,
        nargs="+",
        default=[80],
        help="The number of SGD optimizer iterations for the critic "
        "(default=%(default)s).",
    )
    optimizers_args_group.add_argument(
        "--dynamics-sgd-iter",
        type=int,
        nargs="+",
        default=[80],
        help="The number of SGD optimizer iterations for the dynamics"
        "(default=%(default)s).",
    )
    optimizers_args_group.add_argument(
        "--actor-optimizer",
        type=str,
        nargs="+",
        choices=[
            "Adadelta",
            "Adagrad",
            "Adam",
            "Adamax",
            "Ftrl",
            "Nadam",
            "RMSprop",
            "SGD",
        ],
        default=["Adam"],
        help="The optimizer for the actor network (default=%(default)s).",
    )
    optimizers_args_group.add_argument(
        "--actor-lr",
        type=float,
        nargs="+",
        default=[1e-3],
        help="The learning rate for the actor (policy) optimizer "
        "(default=%(default)s).",
    )
    optimizers_args_group.add_argument(
        "--critic-optimizer",
        type=str,
        nargs="+",
        choices=[
            "Adadelta",
            "Adagrad",
            "Adam",
            "Adamax",
            "Ftrl",
            "Nadam",
            "RMSprop",
            "SGD",
        ],
        default=["Adam"],
        help="The optimizer for the critic (Q-function) network "
        "(default=%(default)s).",
    )
    optimizers_args_group.add_argument(
        "--critic-lr",
        type=float,
        nargs="+",
        default=[1e-3],
        help="The learning rate for the critic (Q-function) optimizer "
        "(default=%(default)s).",
    )
    optimizers_args_group.add_argument(
        "--dynamics-optimizer",
        type=str,
        nargs="+",
        choices=[
            "Adadelta",
            "Adagrad",
            "Adam",
            "Adamax",
            "Ftrl",
            "Nadam",
            "RMSprop",
            "SGD",
        ],
        default=["Adam"],
        help="The optimizer for the dynamics model (default=%(default)s).",
    )
    optimizers_args_group.add_argument(
        "--dynamics-lr",
        type=float,
        nargs="+",
        default=[1e-3],
        help="The learning rate for the dynamics optimizer (default=%(default)s).",
    )
    optimizers_args_group.add_argument(
        "--actor-delay",
        type=int,
        default=80,
        help="The number of traing steps before updating the policy "
        "(default=%(default)s).",
    )
    optimizers_args_group.add_argument(
        "--critic-delay",
        type=int,
        default=1,
        help="The number of traing steps before updating the critic "
        "(default=%(default)s).",
    )
    optimizers_args_group.add_argument(
        "--dynamics-delay",
        type=int,
        default=20,
        help="The number of traing steps before updating the dynamics"
        "(default=%(default)s).",
    )

    exec_args_group = parser.add_argument_group(">> Execution")
    exec_args_group.add_argument(
        "--timesteps-total",
        type=int,
        default=int(1e5),
        help="The total number of timesteps (default=100K).",
    )
    exec_args_group.add_argument(
        "--timesteps-per-iteration",
        type=int,
        default=1024,
        help="The number of timesteps before returning to the trainer "
        "(default=%(default)s).",
    )
    exec_args_group.add_argument(
        "--batch-mode",
        type=str,
        choices=["complete_episodes", "truncate_episodes"],
        default="complete_episodes",
        help="The batch mode (default=%(default)s).",
    )
    exec_args_group.add_argument(
        "--sample-batch-size",
        type=int,
        default=1,
        help="The size of the batch collected by workers (default=%(default)s).",
    )
    exec_args_group.add_argument(
        "--train-batch-size",
        type=int,
        nargs="+",
        default=[2048],
        help="The size of the batch for training (default=%(default)s).",
    )

    resources_args_group = parser.add_argument_group(">> Resources")
    resources_args_group.add_argument(
        "--num-cpus-for-driver",
        type=int,
        default=1,
        help="The number of CPUs to allocate to the driver (default=%(default)s).",
    )
    resources_args_group.add_argument(
        "--num-gpus",
        type=float,
        default=0.0,
        help="The number of GPUs to allocate to the driver (default=%(default)s).",
    )
    resources_args_group.add_argument(
        "--num-workers",
        type=int,
        default=0,
        help="The number of actors used for parallelism (default=%(default)s).",
    )
    resources_args_group.add_argument(
        "--num-cpus-per-worker",
        type=int,
        default=1,
        help="The number of CPUs to allocate to each worker (default=%(default)s).",
    )
    resources_args_group.add_argument(
        "--num-gpus-per-worker",
        type=float,
        default=0.0,
        help="The number of GPUs to allocate to each worker (default=%(default)s).",
    )
    resources_args_group.add_argument(
        "--num-envs-per-worker",
        type=int,
        default=8,
        help="The number of envs per worker used for parallelism "
        "(default=%(default)s).",
    )

    eval_args_group = parser.add_argument_group(">> Evaluation")
    eval_args_group.add_argument(
        "--evaluation-interval",
        type=int,
        default=10,
        help="The interval between training steps for evaluation "
        "(default=%(default)s)",
    )
    eval_args_group.add_argument(
        "--evaluation-num-episodes",
        type=int,
        default=10,
        help="The number of episodes to run per evaluation period "
        "(default=%(default)s)",
    )

    exp_args_group = parser.add_argument_group(">> Experiment")
    exp_args_group.add_argument(
        "--num-samples",
        type=int,
        default=1,
        help="The number of times to sample from the hyperparameter space "
        "(default=%(default)s).",
    )
    exp_args_group.add_argument("--name", type=str, help="The name of the experiment.")

    return parser.parse_args()


def init():
    """Initialize ray framework."""
    ray.init()


def register():
    """Register all agents and custom environtments."""
    register_all_agents()
    register_all_environments()


def make_config(args):
    """Make config dict to trainer."""

    # === Environment ===
    env_config = {"env": tune.grid_search(args.env)}

    # === MAPO ===
    mapo_config = {}
    if args.run in ["MAPO", "OffMAPO"]:
        mapo_config = {
            "branching_factor": tune.grid_search(args.branching_factor),
            "use_true_dynamics": args.use_true_dynamics,
            "model_loss": args.model_loss,
            "madpg_estimator": tune.grid_search(args.madpg_estimator),
        }

    # === Models ===
    models_config = {
        "model": {
            "custom_model": "mapo_model",
            "custom_options": {
                "actor": args.config_actor_net,
                "critic": args.config_critic_net,
            },
        }
    }
    if args.run in ["MAPO", "OffMAPO"]:
        models_config["model"]["custom_model"] = "mapo_model"

        if not args.use_true_dynamics:
            models_config["model"]["custom_options"][
                "dynamics"
            ] = args.config_dynamics_net

    elif args.run == "OurTD3":
        models_config["model"]["custom_model"] = "td3_model"

    # === Preprocessing ===
    preprocessing_config = {"observation_filter": "NoFilter"}

    # === Optimization ===
    optimization_config = {
        "apply_gradients": args.apply_gradients,
        "actor_optimizer": tune.grid_search(args.actor_optimizer),
        "critic_optimizer": tune.grid_search(args.critic_optimizer),
        "actor_lr": tune.grid_search(args.actor_lr),
        "critic_lr": tune.grid_search(args.critic_lr),
        "actor_delay": args.actor_delay,
    }
    if args.run == "MAPO":
        optimization_config["optimizer"] = {"num_sgd_iter": args.num_sgd_iter}

    if args.run in ["MAPO", "OffMAPO"]:
        optimization_config["critic_sgd_iter"] = tune.grid_search(args.critic_sgd_iter)
        optimization_config["critic_delay"] = args.critic_delay

        if not args.use_true_dynamics:
            optimization_config["dynamics_optimizer"] = tune.grid_search(
                args.dynamics_optimizer
            )
            optimization_config["dynamics_sgd_iter"] = tune.grid_search(
                args.dynamics_sgd_iter
            )
            optimization_config["dynamics_lr"] = tune.grid_search(args.dynamics_lr)

    # === Execution ===
    execution_config = {
        "batch_mode": args.batch_mode,
        "sample_batch_size": args.sample_batch_size,
        "train_batch_size": tune.grid_search(args.train_batch_size),
        "timesteps_per_iteration": args.timesteps_per_iteration,
    }

    # === Resources ===
    resources_config = {
        "num_cpus_for_driver": args.num_cpus_for_driver,
        "num_gpus": args.num_gpus,
        "num_workers": args.num_workers,
        "num_cpus_per_worker": args.num_cpus_per_worker,
        "num_gpus_per_worker": args.num_gpus_per_worker,
        "num_envs_per_worker": args.num_envs_per_worker,
    }

    # === Evaluation ===
    eval_config = {}
    if args.run in ["OffMAPO", "OurTD3"]:
        eval_config["evaluation_interval"] = args.evaluation_interval
        eval_config["evaluation_num_episodes"] = args.evaluation_num_episodes

    # === Debugging ===
    debugging_config = {"output": "logdir" if args.debug else None}

    # === TF Session ===
    tf_session_config = {"tf_session_args": {"log_device_placement": False}}

    return {
        **env_config,
        **preprocessing_config,
        **mapo_config,
        **models_config,
        **optimization_config,
        **execution_config,
        **resources_config,
        **eval_config,
        **debugging_config,
        **tf_session_config,
    }


def get_trial_str_creator(config):
    """Returns the trial string creator function."""

    grid_search_config = {}
    for key, value in config.items():
        if isinstance(value, dict) and "grid_search" in value:
            grid_search_config[key] = value["grid_search"]

    def _trial_str_creator(trial):
        """Returns the trial name."""

        short_name = {
            "branching_factor": "bfac",
            "actor_optimizer": "aopt",
            "critic_optimizer": "copt",
            "dynamics_optimizer": "dopt",
            "actor_lr": "alr",
            "critic_lr": "clr",
            "dynamics_lr": "dlr",
            "critic_sgd_iter": "csgd",
            "dynamics_sgd_iter": "dsgd",
            "train_batch_size": "batch",
        }

        sample_id = trial.experiment_tag[: trial.experiment_tag.find("_")]
        tags = trial.experiment_tag[trial.experiment_tag.find("_") + 1 :]

        params = {}
        for param in tags.split(","):
            key_value = param.split("=")
            key, value = key_value[0], key_value[1]
            params[key] = value

        trial_params_list = []
        for key in sorted(params):
            if len(grid_search_config[key]) > 1:
                value = params[key]
                key = short_name.get(key, key)
                trial_params_list.append("{}={}".format(key, value))
        trial_params = ",".join(trial_params_list)

        trainable = trial.trainable_name
        trial_id = trial.trial_id
        trial_str = "{}_{}_{}_{}".format(trial_id, trainable, sample_id, trial_params)
        return trial_str

    return _trial_str_creator


def run(args):
    """Run experiments with configuration given by command-line arguments."""
    trainer = args.run
    experiment = args.name
    config = make_config(args)
    trial_name_creator = get_trial_str_creator(config)

    analysis = tune.run(
        trainer,
        name=experiment,
        verbose=args.verbose,
        stop={"timesteps_total": args.timesteps_total},
        num_samples=args.num_samples,
        config=config,
        trial_name_creator=tune.function(trial_name_creator),
        checkpoint_freq=10,
        checkpoint_at_end=True,
    )

    return analysis


def report(analysis):
    """Report analysis for best configuraion in experiments."""
    best_config = analysis.get_best_config("episode_reward_mean", mode="max")
    best_logdir = analysis.get_best_logdir("episode_reward_mean", mode="max")

    print(">> Best configuration:")
    print(json.dumps(best_config, indent=2, sort_keys=True))
    print()

    print("tensorboard --logdir={}".format(best_logdir))
    print()


if __name__ == "__main__":
    # pylint: disable=invalid-name

    arguments = parse_args()

    init()
    register()

    results = run(arguments)
    report(results)
